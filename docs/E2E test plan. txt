### **E2E Test Plan: Praximous MVP v1.0**

#### **Step 1: Clean Slate (Simulate Fresh Install)**

First, we'll remove any artifacts from previous runs to ensure we're testing the true first-time user experience.

a. **Stop any running containers:**
```bash
docker compose down --volumes
```
*(The `--volumes` flag will remove the old `logs` and `config` data).*

b. **Clean up directories:**
```bash
# On Windows PowerShell:
rm -r -force ./config, ./logs, ./.env

# On Linux/macOS:
# rm -rf ./config ./logs ./.env
```

c. **Clean Python cache (Optional but good practice):**
```bash
python clean_pycache.py
```

#### **Step 2: Setup and Initialization (The "5-Minute Setup")**

We'll now follow the `README.md` Quickstart guide.

a. **Create the environment file:**
```bash
# On Windows PowerShell:
copy .env.template .env

# On Linux/macOS:
# cp .env.template .env
```

b. **Edit the `.env` file:** Open the newly created `.env` file and **add your actual `GEMINI_API_KEY`**. Ensure `OLLAMA_API_URL` is also set correctly.

c. **Run the interactive init command:**
```bash
docker compose run --rm praximous python main.py --init
```
Follow the prompts. You can accept the defaults or enter custom values.

d. **Build and start the service:**
```bash
docker compose up -d --build
```

e. **Verify that the service is running:**
```bash
docker compose ps
```
**Expected Outcome:** You should see the `praximous` container running and the port mapping `0.0.0.0:8000->8000/tcp`.

#### **Step 3: API and Feature Validation**

Now, let's test the running API.

a. **Check the interactive docs:**
Open your web browser and navigate to `http://localhost:8000/docs`.
**Expected Outcome:** You should see the FastAPI interactive documentation page for our API.

b. **Test a local "Smart Skill" (`echo`):**
```bash
curl -X POST http://localhost:8000/api/v1/process -H "Content-Type: application/json" -d '{"task_type": "echo", "prompt": "MVP test successful"}'
```
**Expected Outcome:** You should receive a JSON response with `"status": "success"` and a result containing `"echoed_prompt": "MVP test successful"`.

c. **Test the LLM Router (`default_llm_tasks`):**
```bash
curl -X POST http://localhost:8000/api/v1/process -H "Content-Type: application/json" -d '{"task_type": "default_llm_tasks", "prompt": "What is the speed of light?"}'
```
**Expected Outcome:** You should receive a JSON response with `"status": "success"` and a result from your primary LLM provider (Gemini).

#### **Step 4: Audit Log Validation**

Let's check if the previous requests were logged correctly.

a. **Fetch the analytics data:**
```bash
curl http://localhost:8000/api/v1/analytics
```
**Expected Outcome:** You should see a JSON response containing the two interactions from Step 3, with correct statuses and providers (`skill:echo` and `gemini`).

b. **Test the analytics filter:**
```bash
curl "http://localhost:8000/api/v1/analytics?task_type=echo"
```
**Expected Outcome:** You should see a JSON response containing only the record for the `echo` skill test.

#### **Step 5: Teardown**

Finally, let's stop the application.

a. **Stop the service:**
```bash
docker compose down
```
**Expected Outcome:** The `praximous` container should stop cleanly.